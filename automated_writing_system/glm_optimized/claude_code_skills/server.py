"""
GLMè‡ªåŠ¨åŒ–å†™ä½œç³»ç»Ÿ - MCP Server (ç‹¬ç«‹ç‰ˆæœ¬)
åŸºäºModel Context Protocolå®ç°Claude CodeåŸç”ŸSkills
"""

import asyncio
import json
import os
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
import sys

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# MCP imports
try:
    from mcp.server import Server
    from mcp.server.models import InitializationOptions
    from mcp.server.stdio import stdio_server
    from mcp.types import (
        Resource, Tool, TextContent, ImageContent, EmbeddedResource,
        LoggingLevel
    )
    MCP_AVAILABLE = True
except ImportError:
    logger.error("MCP library not found. Please install: pip install mcp")
    MCP_AVAILABLE = False

# GLM imports
try:
    from zhipuai import ZhipuAI
    GLM_AVAILABLE = True
except ImportError:
    logger.error("GLM library not found. Please install: pip install zhipuai")
    GLM_AVAILABLE = False

class GLMMCPServer:
    """GLM MCPæœåŠ¡å™¨"""

    def __init__(self):
        if not MCP_AVAILABLE or not GLM_AVAILABLE:
            raise ImportError("Required libraries not installed")

        self.server = Server("glm-writing-system")
        self.client = None
        self._setup_client()
        self._register_tools()

    def _setup_client(self):
        """è®¾ç½®GLMå®¢æˆ·ç«¯"""
        api_key = os.getenv("GLM_API_KEY")
        if not api_key:
            raise ValueError("GLM_API_KEY environment variable is required")
        self.client = ZhipuAI(api_key=api_key)

    def _register_tools(self):
        """æ³¨å†Œæ‰€æœ‰å·¥å…·"""

        @self.server.list_tools()
        async def handle_list_tools() -> List[Tool]:
            """åˆ—å‡ºæ‰€æœ‰å¯ç”¨å·¥å…·"""
            return [
                Tool(
                    name="glm_generate",
                    description="ä½¿ç”¨GLMç”Ÿæˆå†…å®¹ï¼ˆæ–‡ç« ã€ä»£ç ã€æ–‡æœ¬ç­‰ï¼‰",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "prompt": {
                                "type": "string",
                                "description": "ç”Ÿæˆå†…å®¹çš„æç¤ºè¯"
                            },
                            "model": {
                                "type": "string",
                                "enum": ["glm-4.6", "glm-4.5", "glm-3-turbo"],
                                "description": "ä½¿ç”¨çš„æ¨¡å‹",
                                "default": "glm-4.5"
                            },
                            "max_tokens": {
                                "type": "integer",
                                "description": "æœ€å¤§tokenæ•°",
                                "default": 2000
                            }
                        },
                        "required": ["prompt"]
                    }
                ),
                Tool(
                    name="glm_analyze",
                    description="åˆ†ææ–‡æœ¬ï¼ˆé£æ ¼ã€æƒ…æ„Ÿã€å…³é”®è¯ç­‰ï¼‰",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "text": {
                                "type": "string",
                                "description": "è¦åˆ†æçš„æ–‡æœ¬"
                            },
                            "analysis_type": {
                                "type": "string",
                                "enum": ["style", "sentiment", "keywords", "summary"],
                                "description": "åˆ†æç±»å‹",
                                "default": "summary"
                            }
                        },
                        "required": ["text"]
                    }
                ),
                Tool(
                    name="glm_optimize",
                    description="ä¼˜åŒ–æ–‡æœ¬æˆ–prompt",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "content": {
                                "type": "string",
                                "description": "è¦ä¼˜åŒ–çš„å†…å®¹"
                            },
                            "goal": {
                                "type": "string",
                                "enum": ["clarity", "conciseness", "engagement", "seo"],
                                "description": "ä¼˜åŒ–ç›®æ ‡",
                                "default": "clarity"
                            }
                        },
                        "required": ["content"]
                    }
                ),
                Tool(
                    name="glm_translate",
                    description="ç¿»è¯‘æ–‡æœ¬",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "text": {
                                "type": "string",
                                "description": "è¦ç¿»è¯‘çš„æ–‡æœ¬"
                            },
                            "target_language": {
                                "type": "string",
                                "description": "ç›®æ ‡è¯­è¨€",
                                "default": "English"
                            }
                        },
                        "required": ["text"]
                    }
                ),
                Tool(
                    name="glm_code",
                    description="ç”Ÿæˆæˆ–è§£é‡Šä»£ç ",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "request": {
                                "type": "string",
                                "description": "ä»£ç ç”Ÿæˆæˆ–è§£é‡Šè¯·æ±‚"
                            },
                            "language": {
                                "type": "string",
                                "description": "ç¼–ç¨‹è¯­è¨€",
                                "default": "Python"
                            }
                        },
                        "required": ["request"]
                    }
                )
            ]

        @self.server.call_tool()
        async def handle_call_tool(name: str, arguments: Dict[str, Any]) -> List[TextContent]:
            """å¤„ç†å·¥å…·è°ƒç”¨"""
            try:
                if name == "glm_generate":
                    return await self._handle_generate(arguments)
                elif name == "glm_analyze":
                    return await self._handle_analyze(arguments)
                elif name == "glm_optimize":
                    return await self._handle_optimize(arguments)
                elif name == "glm_translate":
                    return await self._handle_translate(arguments)
                elif name == "glm_code":
                    return await self._handle_code(arguments)
                else:
                    return [TextContent(
                        type="text",
                        text=f"Unknown tool: {name}"
                    )]
            except Exception as e:
                logger.error(f"Error in {name}: {e}")
                return [TextContent(
                    type="text",
                    text=f"Error: {str(e)}"
                )]

    async def _handle_generate(self, args: Dict[str, Any]) -> List[TextContent]:
        """å¤„ç†ç”Ÿæˆè¯·æ±‚"""
        prompt = args["prompt"]
        model = args.get("model", "glm-4.5")
        max_tokens = args.get("max_tokens", 2000)

        response = self.client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†™ä½œåŠ©æ‰‹"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=max_tokens
        )

        content = response.choices[0].message.content

        return [TextContent(
            type="text",
            text=f"âœ… ç”ŸæˆæˆåŠŸï¼ˆæ¨¡å‹ï¼š{model}ï¼‰\n\n{content}"
        )]

    async def _handle_analyze(self, args: Dict[str, Any]) -> List[TextContent]:
        """å¤„ç†åˆ†æè¯·æ±‚"""
        text = args["text"]
        analysis_type = args.get("analysis_type", "summary")

        prompts = {
            "style": "è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„å†™ä½œé£æ ¼å’Œç‰¹ç‚¹",
            "sentiment": "è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘",
            "keywords": "è¯·æå–ä»¥ä¸‹æ–‡æœ¬çš„ä¸»è¦å…³é”®è¯",
            "summary": "è¯·æ€»ç»“ä»¥ä¸‹æ–‡æœ¬çš„ä¸»è¦å†…å®¹"
        }

        prompt = f"{prompts.get(analysis_type, prompts['summary'])}:\n\n{text}"

        response = self.client.chat.completions.create(
            model="glm-4.5",
            messages=[
                {"role": "user", "content": prompt}
            ]
        )

        return [TextContent(
            type="text",
            text=f"ğŸ“Š {analysis_type}åˆ†æç»“æœï¼š\n\n{response.choices[0].message.content}"
        )]

    async def _handle_optimize(self, args: Dict[str, Any]) -> List[TextContent]:
        """å¤„ç†ä¼˜åŒ–è¯·æ±‚"""
        content = args["content"]
        goal = args.get("goal", "clarity")

        goal_descriptions = {
            "clarity": "è®©å†…å®¹æ›´æ¸…æ™°æ˜“æ‡‚",
            "conciseness": "è®©å†…å®¹æ›´ç®€æ´ç²¾ç‚¼",
            "engagement": "è®©å†…å®¹æ›´å¸å¼•äºº",
            "seo": "è®©å†…å®¹æ›´é€‚åˆSEO"
        }

        prompt = f"è¯·ä¼˜åŒ–ä»¥ä¸‹å†…å®¹ï¼Œç›®æ ‡ï¼š{goal_descriptions.get(goal, goal)}\n\nåŸå†…å®¹ï¼š\n{content}"

        response = self.client.chat.completions.create(
            model="glm-4.5",
            messages=[
                {"role": "user", "content": prompt}
            ]
        )

        return [TextContent(
            type="text",
            text=f"âœ¨ ä¼˜åŒ–ç»“æœï¼ˆç›®æ ‡ï¼š{goal}ï¼‰ï¼š\n\n{response.choices[0].message.content}"
        )]

    async def _handle_translate(self, args: Dict[str, Any]) -> List[TextContent]:
        """å¤„ç†ç¿»è¯‘è¯·æ±‚"""
        text = args["text"]
        target_language = args.get("target_language", "English")

        prompt = f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_language}ï¼š\n\n{text}"

        response = self.client.chat.completions.create(
            model="glm-4.5",
            messages=[
                {"role": "user", "content": prompt}
            ]
        )

        return [TextContent(
            type="text",
            text=f"ğŸŒ ç¿»è¯‘ç»“æœï¼ˆ{target_language}ï¼‰ï¼š\n\n{response.choices[0].message.content}"
        )]

    async def _handle_code(self, args: Dict[str, Any]) -> List[TextContent]:
        """å¤„ç†ä»£ç è¯·æ±‚"""
        request = args["request"]
        language = args.get("language", "Python")

        prompt = f"è¯·ç”¨{language}å¤„ç†ä»¥ä¸‹è¯·æ±‚ï¼š\n\n{request}\n\nè¯·æä¾›ä»£ç å’Œå¿…è¦çš„è§£é‡Šã€‚"

        response = self.client.chat.completions.create(
            model="glm-4.6",  # ä»£ç ç”Ÿæˆä½¿ç”¨æœ€å¼ºæ¨¡å‹
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹"},
                {"role": "user", "content": prompt}
            ]
        )

        return [TextContent(
            type="text",
            text=f"ğŸ’» ä»£ç ç”Ÿæˆï¼ˆ{language}ï¼‰ï¼š\n\n{response.choices[0].message.content}"
        )]

    async def run(self):
        """è¿è¡ŒæœåŠ¡å™¨"""
        if not self.client:
            raise ValueError("GLM client not initialized")

        async with stdio_server() as (read_stream, write_stream):
            await self.server.run(
                read_stream,
                write_stream,
                InitializationOptions(
                    server_name="glm-writing-system",
                    server_version="1.0.0",
                    capabilities=self.server.get_capabilities(
                        notification_options=None,
                        experimental_capabilities={}
                    )
                )
            )

async def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        if not os.getenv("GLM_API_KEY"):
            print("âŒ é”™è¯¯ï¼šæœªè®¾ç½® GLM_API_KEY ç¯å¢ƒå˜é‡")
            print("\nè¯·è®¾ç½®æ‚¨çš„GLM API Key:")
            print("export GLM_API_KEY='your-api-key'")
            print("\nè·å–API Key: https://open.bigmodel.cn/")
            return

        # åˆ›å»ºå¹¶è¿è¡ŒæœåŠ¡å™¨
        server = GLMMCPServer()
        logger.info("GLM MCP Serverå¯åŠ¨æˆåŠŸ")
        await server.run()

    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        print("\nè¯·å®‰è£…å¿…è¦çš„ä¾èµ–:")
        print("pip install mcp zhipuai")
    except Exception as e:
        logger.error(f"Server error: {e}")
        print(f"âŒ æœåŠ¡å™¨é”™è¯¯: {e}")

if __name__ == "__main__":
    asyncio.run(main())